# Lecture 1. Introduction 
- 컴퓨터 그래픽과 통계적 생성모형 둘 다 사전지식과 데이터를 이용해 이미지를 생성하지만, 컴퓨터 그래픽은 사전지식을, 통계적 생성모형은 데이터를 더 많이 사용한다.
- 생성모형에서 핵심질문
  - 표현: 많은 확률변수들의 결합분포를 어떻게 모델링할 것인가?
  - 학습: 확률분포를 비교하는 올바른 방법은 무엇인가?
  - 추론: 생성과정으로부터 어떻게 추론을 수행할 것인가?

# Lecture 2. Background
- 주어진 훈련 데이터가 개 이미지라고 하면, 생성모형에서 학습된 확률분포는
  - 새로운 개 이미지를 생성할 수 있어야 한다. (sampling)
  - 이미지가 개이면 확률이 높고 아니면 확률이 낮아야 한다. (anomaly detection) 
  - 이미지의 특징을 학습해야 한다. (feature)
- Q1.생성모형을 학습할 때 손실함수로 $d(P_{data},P_{\theta})$을 계산하는데, 여기서 $P_{data}$는 모짖단의 확률분포인지,
아니면 훈련 데이터셋의 확률분포인지?
- 기본적인 이산확률분포
  - Bernoulli 분포: 편향된 동전 던지기
  - caterorical 분포: 편향된 주사위 굴리기
- 결합확률분포 예제
  - 하나의 RGB 픽셀은 R,G,B 채널이 각각 256개(0...255)의 값을 가질 수 있으므로, 결합확률분포 $p(R=r,G=g,B=g)$를 기술하는데 256*256*256-1개의 파라미터가 필요함 
  - 베르누이 결합확률분포 $P(X_1,...X_n)$는 $2^n-1$개의 파라미터가 필요하다. n개의 픽셀로 이루어진 흑백 이미지$2^n$개를 생성할 수 있다.
- 확률분포가 독립이라고 가정하면
  - $X_1,...X_n$이 서로 독립이면, $P(X_1,...X_n)=P(X_1)P(X_2)...P(X_n)$
  - $2^n$개의 상태를 표현하는데 $n$개의 파라미터가 필요하다.
  - 하지만 가정이 너무 강해서 잘 맞지 않다.(픽셀간의 상관관계를 고려하지 않는다)
- 두가지 중요한 확률 법칙
  - 연쇄 법칙: $P(X_1,X_2,...,X_n)=p(X_1)P(X_2|X_1)...P(X_n|X_1,X_2,...,X_{n-1})$
  - 베이즈 법칙: $p(X1|X2)=\frac{P(X1,X2)}{P(X2)}=\frac{P(X2|X1)P(X1)}{P(X2)}$
- 조건부 독립
  - 연쇄율은 파라미터수를 줄여주지 않는다.
  - 왜냐하면 $P(X_1,...,X_n)=P(X_1)P(X_2|X_1)P(X_3|X_1,X_2)...P(X_n|X_1,X_2,...,X_{n-1})$ 이므로 $1+2+...+2^{n-1}=2^n-1$개의 파라미터가 필요하다.
  - 만약 $X_i+1$과 $X_1,...,X_{i-1}|X_i$이 독립이라고 가정하면 $P(X_1,...,X_n)=P(X_1)P(X_2|X_1)P(X_3|X_2)...P(X_n|X_{n-1})$ 이므로 파라미터 수가 $1+2(n-1)=2n-1$가 된다.
- Bayes 네트워크  
  - 결합확률분포를 조건부확률로 표현 (연쇄율에서 조건부 독립을 가정)
  - $P(X_1,...,X_n)=\prod{P(X_i|X_{A_i}}$
  - 유향 비순환 그래프 (directed acyclic graph; DAG)로 기술할 수 있다.
  - $G(V,E)$에서 노드 $V$는 확률변수를, 에지 $E$는 조건부 확률분포를 나타낸다.
- Bayes 네트워크 예제 (슬라이드 13)
  - $p(d,i,g,s,l)=p(d)p(i)p(d|i,d)p(s|i)p(l|g)$
  
